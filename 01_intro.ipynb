{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1bb00f-d306-475a-91e7-b45000bc08fa",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f5ccd-242f-40da-8bee-ff021953cfa8",
   "metadata": {},
   "source": [
    "## .nemo file (unpacked)\n",
    "\n",
    "Assuming you have run and read `test-nemo-mwe.py`, the model that is loaded via the `stt_en_fastconformer_ctc_large.nemo` consists of a bundle of files. Let's unpack what is contained in `stt_en_fastconformer_ctc_large.nemo` into `tmp/nemo_unpacked`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eabd251-a3ad-47a1-9753-0be5f0f746db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=10aal_afHODYOA6qh5IQX8IGnWbfU3Nhd\n",
      "From (redirected): https://drive.google.com/uc?id=10aal_afHODYOA6qh5IQX8IGnWbfU3Nhd&confirm=t&uuid=c8f0cfa0-dab8-42e5-b6b3-fed3ababfd9c\n",
      "To: /home/naysan/git-repos/fastconformer_standalone/tmp/nemo_unpacked.tgz\n",
      "100%|██████████| 463M/463M [00:04<00:00, 99.6MB/s] \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gdown 10aal_afHODYOA6qh5IQX8IGnWbfU3Nhd -O tmp/\n",
    "tar -xf tmp/nemo_unpacked.tgz -C tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef9416-6269-4538-a0b3-133649e59543",
   "metadata": {},
   "source": [
    "We can see from the file list below that there is a model checkpoint (`model_weights.ckpt`) and various files relating to the model configuration (`model_config.yaml`, `*_vocab.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b48f30-cea3-432f-96cf-055b6f25e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 442M\n",
      "-rwxrwxr-x 1 naysan naysan 255K Sep  5 15:26 0124d42f914e45e98c214bd5afd17f55_tokenizer.model\n",
      "-rwxrwxr-x 1 naysan naysan 5.9K Sep  5 15:26 129be3e4b71e449e86261ee42b6849fa_vocab.txt\n",
      "-rwxrwxr-x 1 naysan naysan  17K Sep  5 15:26 7561592dab144ebaaade5d1244a9ffb0_tokenizer.vocab\n",
      "-rw-r--r-- 1 naysan naysan  15K Sep  5 15:26 model_config.yaml\n",
      "-rw-r--r-- 1 naysan naysan 442M Sep  5 15:26 model_weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh tmp/nemo_unpacked/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c9850-580f-4846-ad09-88e27543b712",
   "metadata": {},
   "source": [
    "## Load pre-trained model\n",
    "\n",
    "Let's load the pre-trained model from the `.nemo` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8acb6892-dee7-448d-b852-567a70b7dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-09-13 18:11:46 mixins:170] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-09-13 18:11:47 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2023-09-13 18:11:47 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2023-09-13 18:11:47 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-09-13 18:11:47 features:289] PADDING: 0\n",
      "[NeMo I 2023-09-13 18:11:47 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/naysan/git-repos/fastconformer_standalone/tmp/stt_en_fastconformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(\"tmp/stt_en_fastconformer_ctc_large.nemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7dfd9-56f3-4b86-938f-0d553fb63163",
   "metadata": {},
   "source": [
    "## Model structure\n",
    "\n",
    "We can see from the PyTorch model print-out below that it is composed of various sub-components:\n",
    "- preprocessor\n",
    "- encoder\n",
    "    - pre_encode\n",
    "    - pos_enc\n",
    "    - layers\n",
    "    - decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f0edfa-c69f-4e2f-ab8d-c86b8d51b225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(512, 512, kernel_size=(9,), stride=(1,), groups=512)\n",
       "          (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(512, 1025, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (_wer): WERBPE()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c13edb-6e41-465a-bf1b-cd6457ab062b",
   "metadata": {},
   "source": [
    "## Call each component one at a time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90381459-6590-4ae6-9e1c-8f664550e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "samples, sample_rate = torchaudio.load(\"tmp/cat.wav\")\n",
    "\n",
    "audio_samples = samples\n",
    "audio_lens    = torch.tensor(samples.shape[1]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7842e-b3fe-4a7c-adaf-e818de34e601",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59767a52-9b4c-4581-aed7-28775e7adb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(asr_model.preprocessor.featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4c9530-dab5-4c2a-8345-4641efc68a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbank_feats, fbank_lens = asr_model.preprocessor.featurizer(audio_samples.to('cuda'), audio_lens.to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54897d40-8d5e-4683-8f3d-95d8f23acd12",
   "metadata": {},
   "source": [
    "### Pre-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec92d22-8783-43b7-a2c8-7be8c0eacd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "preenc_feats, preenc_lens = asr_model.encoder.pre_encode(\n",
    "    fbank_feats.transpose(1,2),\n",
    "    fbank_lens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef647a0-1031-4903-a862-b2f43626621a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nemo.collections.asr.parts.submodules.multi_head_attention.RelPositionalEncoding"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(asr_model.encoder.pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76739f5c-6fd0-472c-a348-15804082ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preenc_feats, pos_emb = asr_model.encoder.pos_enc(preenc_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e693e-7efa-4ced-82fd-851257e4543f",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07024439-d342-4fe8-ae1e-afc5f968663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nemo.collections.asr.modules.conformer_encoder.ConformerEncoder"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(asr_model.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c25c7-5194-47dc-9e07-ef4237910560",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76bddb2b-8bb3-4fcd-a981-6c1f911e52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask, att_mask = asr_model.encoder._create_masks(\n",
    "    att_context_size=asr_model.encoder.att_context_size,\n",
    "    padding_length=preenc_lens,\n",
    "    max_audio_length=preenc_feats.size(1),\n",
    "    offset=None,\n",
    "    device=preenc_feats.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab98ab3-ba63-4f4f-941e-f4ccabf08ad2",
   "metadata": {},
   "source": [
    "#### Pass through encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c0ebda0-1d3e-478a-9a3b-a82fb2dd10e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nemo.collections.asr.parts.submodules.conformer_modules.ConformerLayer"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(asr_model.encoder.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f8ea683-9e55-490c-b4c8-5c0104272489",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signal = preenc_feats\n",
    "\n",
    "for lth, (drop_prob, layer) in enumerate(zip(asr_model.encoder.layer_drop_probs, asr_model.encoder.layers)):\n",
    "    original_signal = audio_signal\n",
    "\n",
    "    audio_signal = layer(\n",
    "        x=audio_signal,\n",
    "        att_mask=att_mask,\n",
    "        pos_emb=pos_emb,\n",
    "        pad_mask=pad_mask,\n",
    "        cache_last_channel=None,\n",
    "        cache_last_time=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2681c8-0efb-4b01-96cf-39d29b46f862",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0326c573-f68a-43b4-bc22-d4f3ab7b06de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nemo.collections.asr.modules.conv_asr.ConvASRDecoder"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(asr_model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1becf1b6-1e93-4e37-89e1-b7cfb752cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = asr_model.decoder.decoder_layers(\n",
    "    audio_signal.transpose(1,2)\n",
    ").transpose(1, 2)\n",
    "\n",
    "log_probs = torch.nn.functional.log_softmax(decoder_output, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de046d1-a3e3-4de8-b594-f49942ebaf90",
   "metadata": {},
   "source": [
    "#### Get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2306ed4c-a8b9-41dc-973d-05dec297e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_predictions = log_probs.argmax(dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b41cba60-f2ba-4b63-847d-5aa68c54d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hypotheses, all_hyp = asr_model.decoding.ctc_decoder_predictions_tensor(\n",
    "    log_probs,\n",
    "    decoder_lengths=preenc_lens,\n",
    "    return_hypotheses=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "922ee67b-c81b-4534-bcb1-78669d902952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8fc23-4a20-4a75-9647-4383124e18c0",
   "metadata": {},
   "source": [
    "## Try to manually configure components (see if we can get same result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cf93f-1695-4946-8fac-d680f824d5e5",
   "metadata": {},
   "source": [
    "### Get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea04523e-6317-4da6-9703-1ace380ecfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('tmp/nemo_unpacked/model_config.yaml', 'r') as file:\n",
    "    model_config = yaml.safe_load(file)\n",
    "\n",
    "def isolate_target_key(config_dict):\n",
    "    return {\n",
    "        'target' : config_dict['_target_'],\n",
    "        'config': dict([ (k,v) for (k,v) in config_dict.items() if k != '_target_' ])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05edaad-dfd7-4a02-82bc-1556397765b8",
   "metadata": {},
   "source": [
    "### Get model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d72161c0-4291-49c7-bc97-1901228e583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "weights = torch.load(\"tmp/nemo_unpacked/model_weights.ckpt\")\n",
    "\n",
    "list(weights.keys())[:10]\n",
    "\n",
    "encoder_weights = OrderedDict([ (k.replace('encoder.', ''),v) for (k,v) in weights.items() if k.startswith('encoder') ])\n",
    "decoder_weights = OrderedDict([ (k.replace('decoder.', ''),v) for (k,v) in weights.items() if k.startswith('decoder') ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7e54d-9c62-41ad-8948-22581a70d491",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c256192-b85e-4325-b1c5-73b2ad5530a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 'nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor',\n",
       " 'config': {'sample_rate': 16000,\n",
       "  'normalize': 'per_feature',\n",
       "  'window_size': 0.025,\n",
       "  'window_stride': 0.01,\n",
       "  'window': 'hann',\n",
       "  'features': 80,\n",
       "  'n_fft': 512,\n",
       "  'log': True,\n",
       "  'frame_splicing': 1,\n",
       "  'dither': 1e-05,\n",
       "  'pad_to': 0,\n",
       "  'pad_value': 0.0}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fext = isolate_target_key(model_config['preprocessor'])\n",
    "\n",
    "fext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d9af082-680c-465b-8007-3ad9ec39c4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-09-13 18:38:41 features:289] PADDING: 0\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.modules import AudioToMelSpectrogramPreprocessor\n",
    "\n",
    "fext = AudioToMelSpectrogramPreprocessor(**fext['config']).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1995b062-b93c-43ee-91c4-7056e6874efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8675, -0.2694, -0.1895,  ..., -0.1655, -0.8155, -1.7735],\n",
       "          [-1.6972, -1.2183, -1.0287,  ..., -0.7180, -0.8771, -1.5676],\n",
       "          [-1.7146, -1.3714, -1.3406,  ..., -1.0109, -1.5163, -0.8815],\n",
       "          ...,\n",
       "          [-1.6872, -1.1314, -0.7925,  ..., -1.0016, -0.9704, -1.1705],\n",
       "          [-1.7708, -1.0728, -0.9095,  ..., -1.0243, -0.9215, -1.0464],\n",
       "          [-1.3496, -1.1106, -0.3422,  ..., -0.1715, -0.7981, -0.8456]]],\n",
       "        device='cuda:0'),\n",
       " tensor([53], device='cuda:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_fbank_feats, manual_fbank_lens = fext.featurizer(audio_samples.to('cuda'), audio_lens.to('cuda'))\n",
    "\n",
    "(manual_fbank_feats, manual_fbank_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c750e0-d888-4fa5-aed9-565ffba64018",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "815be55e-5ce9-44a3-8a3e-fd49c1ef75e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 'nemo.collections.asr.modules.ConformerEncoder',\n",
       " 'config': {'feat_in': 80,\n",
       "  'feat_out': -1,\n",
       "  'n_layers': 18,\n",
       "  'd_model': 512,\n",
       "  'subsampling': 'dw_striding',\n",
       "  'subsampling_factor': 8,\n",
       "  'subsampling_conv_channels': 256,\n",
       "  'causal_downsampling': False,\n",
       "  'ff_expansion_factor': 4,\n",
       "  'self_attention_model': 'rel_pos',\n",
       "  'n_heads': 8,\n",
       "  'att_context_size': [-1, -1],\n",
       "  'att_context_style': 'regular',\n",
       "  'xscaling': True,\n",
       "  'untie_biases': True,\n",
       "  'pos_emb_max_len': 5000,\n",
       "  'conv_kernel_size': 9,\n",
       "  'conv_norm_type': 'batch_norm',\n",
       "  'conv_context_size': None,\n",
       "  'dropout': 0.1,\n",
       "  'dropout_pre_encoder': 0.1,\n",
       "  'dropout_emb': 0.0,\n",
       "  'dropout_att': 0.1}}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = isolate_target_key(model_config['encoder'])\n",
    "\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1b1c89a3-d41e-4b18-9415-dbb6ce5e8dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo.collections.asr.modules import ConformerEncoder\n",
    "\n",
    "enc = ConformerEncoder(**enc['config']).to('cuda')\n",
    "enc.load_state_dict(encoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ec973204-f49d-421d-8b06-7e84826ed3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_preenc_feats, manual_preenc_lens = asr_model.encoder.pre_encode(\n",
    "    manual_fbank_feats.transpose(1,2),\n",
    "    manual_fbank_lens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0037c500-5b58-4a33-90e6-9d0ff006e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_preenc_feats, manual_pos_emb = enc.pos_enc(manual_preenc_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e017258b-299d-4d47-b242-9f3506151211",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pad_mask, manual_att_mask = enc._create_masks(\n",
    "    att_context_size=enc.att_context_size,\n",
    "    padding_length=manual_preenc_lens,\n",
    "    max_audio_length=manual_preenc_feats.size(1),\n",
    "    offset=None,\n",
    "    device=manual_preenc_feats.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac86f5a4-180c-403a-b5ac-74aba3300dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signal = manual_preenc_feats\n",
    "\n",
    "for lth, (drop_prob, layer) in enumerate(zip(enc.layer_drop_probs, enc.layers)):\n",
    "    original_signal = audio_signal\n",
    "\n",
    "    audio_signal = layer(\n",
    "        x=audio_signal,\n",
    "        att_mask=manual_att_mask,\n",
    "        pos_emb=manual_pos_emb,\n",
    "        pad_mask=manual_pad_mask,\n",
    "        cache_last_channel=None,\n",
    "        cache_last_time=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ae9a4-8c4d-4fd6-b295-cc8ab8f2f32c",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "88ce8016-286f-4562-8c42-733a074d2ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nemo.collections.asr.modules.ConvASRDecoder'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = isolate_target_key(model_config['decoder'])\n",
    "\n",
    "dec['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32c47a5f-7c28-4e5c-b3e2-5b4dcfbf1b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nemo.collections.asr.modules import ConvASRDecoder\n",
    "\n",
    "dec = ConvASRDecoder(**dec['config']).to('cuda')\n",
    "dec.load_state_dict(decoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ecc3dac3-4d9d-4e95-80f7-288a7de1c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_decoder_output = dec.decoder_layers(\n",
    "    audio_signal.transpose(1,2)\n",
    ").transpose(1, 2)\n",
    "\n",
    "manual_log_probs = torch.nn.functional.log_softmax(manual_decoder_output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae8815f6-365a-46e6-93eb-883c116517c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_greedy_predictions = manual_log_probs.argmax(dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7b830efe-c6fa-4ab3-b11e-18729012cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just use same decoder as NeMo-configured model above for convenience\n",
    "manual_current_hypotheses, manual_all_hyp = asr_model.decoding.ctc_decoder_predictions_tensor(\n",
    "    manual_log_probs,\n",
    "    decoder_lengths=manual_preenc_lens,\n",
    "    return_hypotheses=False\n",
    ")\n",
    "\n",
    "manual_current_hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c38eeb-5843-4e52-826d-c574d3aa18e2",
   "metadata": {},
   "source": [
    "Close enough! (Would expect something very different/junk if model was not working...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b9dac-a773-4763-a645-d9b3a2d9df55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
